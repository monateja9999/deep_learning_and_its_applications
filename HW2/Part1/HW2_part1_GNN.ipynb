{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52aac0c151b3e181",
   "metadata": {},
   "source": [
    "# HW2 part 2: GNN \n",
    "\n",
    "## Graph Attention Network (GAT) Implementation for Node Classification\n",
    "\n",
    "**Objective:** Implement a GAT from scratch to perform node classification using an OGB dataset. Develop neural components, including the forward pass, as well as training and testing routines. There are 17 `todo`s and 2 questions for GAT.\n",
    "\n",
    "A Graph Attention Network (GAT) applies the concept of self-attention to graph-structured data. Unlike traditional Graph Convolutional Networks (GCNs) that rely on fixed or uniform weights derived from adjacency structures, GAT learns to assign different weights (attention scores) to different edges. This allows the model to focus more on important neighbors while possibly ignoring less relevant ones. By doing so, GAT effectively captures how each node interacts with its neighbors in a more flexible and adaptive way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6dfc94c7bb788a68",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-10T19:47:13.575637Z",
     "start_time": "2025-03-10T19:47:06.726046Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# This notebook's first part demonstrates how to train and evaluate a GAT model on the ogbn-arxiv dataset for node classification (To predict the category of each paper). Make sure you know the basics of GNNs, PyG and pytorch before starting this notebook. If not, please check the corresponding tutorials first.\n",
    "\n",
    "# If you use Google Colab, you can uncomment and run the following command to install the required packages.\n",
    "# For this assigment, we recommend using your own local computer since the RAM usage is high, Colab may crash due to the high RAM usage.\n",
    "# We passed our assignment on our local computers using python version 3.10.16.\n",
    "# !pip install torch==2.5.0\n",
    "# !pip install torch-geometric==2.6.1\n",
    "# !pip install ogb==1.3.6\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch_geometric.nn import GATConv\n",
    "from ogb.nodeproppred import PygNodePropPredDataset,Evaluator\n",
    "\n",
    "# We recommend to use GPU for training if possible. Because GAT is a relatively large model, training on CPU can be slow. But after testing, we find that training GAT on CPU is also acceptable.\n",
    "\n",
    "\n",
    "# if torch.cuda.is_available():\n",
    "#     device = torch.device(\"cuda\")  # NVIDIA GPU\n",
    "# else:\n",
    "#     device = torch.device(\"cpu\")   # CPU fallback\n",
    "device = torch.device(\"cpu\")   # CPU fallback\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "\n",
    "import random\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    # make cudnn deterministic\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# set a random seed for reproducibility\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d19bf43fc6e61d5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-10T19:47:13.660163Z",
     "start_time": "2025-03-10T19:47:13.586988Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of graphs in dataset: 1\n",
      "Number of features: 128, Number of classes: 40\n",
      "torch.Size([169343, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\monat\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\ogb\\nodeproppred\\dataset_pyg.py:69: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.data, self.slices = torch.load(self.processed_paths[0])\n",
      "c:\\Users\\monat\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# ## Define a GAT Model for Node Classification\n",
    "\n",
    "#### Model Structure\n",
    "\n",
    "## The model is divided into two GATConv layers:\n",
    "##   The first layer uses multi-head attention (specified by num_heads) to produce richer representations. Here, each head outputs hidden_channels features, and they are concatenated, resulting in a total output dimension of hidden_channels * num_heads.\n",
    "##   The second layer is a single-head output layer that directly predicts the final node embeddings or categories. Its output dimension corresponds to the number of classes (out_channels).\n",
    "\n",
    "#### Layer Normalization\n",
    "\n",
    "## After the first GAT layer, the output is passed through nn.LayerNorm(hidden_channels * num_heads). This helps stabilize training by normalizing feature distributions across different nodes.\n",
    "\n",
    "#### Forward Pass\n",
    "\n",
    "## First apply dropout to the input features (x) to reduce overfitting.\n",
    "## Pass the data through the first GAT layer (gat1).\n",
    "## Apply an ELU activation, then layer normalization, and another dropout.\n",
    "## Finally, pass the features through the second GAT layer (gat2) to get the predictions (the paper category with the highest score).\n",
    "\n",
    "class GAT(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_heads=8, dropout=0.6,add_self_loops=True):\n",
    "        super(GAT, self).__init__()\n",
    "        self.dropout = dropout\n",
    "        # The first layer: multi-head attention, each head outputs hidden_channels, and the total output dimension = hidden_channels * num_heads (i.e. the input dimension of the second layer), you don't need to care the concatenation in this layer\n",
    "        # The usage of GATConv is: GATConv(in_channels, out_channels, heads=XXX, concat=XXX, dropout=dropout, add_self_loops=XXX)\n",
    "        ## TODO 1: Define the first GAT layer (2 points)\n",
    "\n",
    "        self.gat1 = GATConv(in_channels, hidden_channels, heads=num_heads, concat=True, dropout=dropout, add_self_loops=add_self_loops)\n",
    "        \n",
    "        ## TODO 1: Define the first GAT layer\n",
    "\n",
    "        # The second layer: single-head output, not concatenated, directly output the dimension of the node categories\n",
    "        ## TODO 2: Define the second GAT layer (2 points)\n",
    "\n",
    "        self.gat2 = GATConv(hidden_channels * num_heads, out_channels, heads=1, concat=False, dropout=dropout, add_self_loops=add_self_loops)\n",
    "        \n",
    "        ## TODO 2: Define the second GAT layer\n",
    "\n",
    "        # Layer normalization for stable training, use nn.LayerNorm, the input is the hidden_channels * num_heads\n",
    "        ## TODO 3: Define a LayerNorm layer (2 points)\n",
    "        \n",
    "        self.layer_norm = nn.LayerNorm(hidden_channels * num_heads)\n",
    "\n",
    "        ## TODO 3: Define a LayerNorm layer\n",
    "\n",
    "    def forward(self, data):\n",
    "        # data contains x and edge_index\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "\n",
    "        ## TODO 4: use a dropout layer for the input features, then apply the first GAT layer (2 points)\n",
    "        # use F.dropout to perform dropout, the input is x, and the dropout rate is self.dropout, set training=self.training\n",
    "\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.gat1(x, edge_index)\n",
    "\n",
    "        ## TODO 4: use a dropout layer for the input features, then apply the first GAT layer\n",
    "\n",
    "        ## TODO 5: apply ELU activation, layer normalization and dropout (2 points)\n",
    "        # use F.elu for ELU activation, the input is x\n",
    "\n",
    "        x = F.elu(x)\n",
    "        x = self.layer_norm(x)\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "\n",
    "        ## TODO 5: apply ELU activation, layer normalization and dropout\n",
    "\n",
    "        ## TODO 6: apply the second GAT layer (2 points)\n",
    "\n",
    "        x = self.gat2(x, edge_index)\n",
    "\n",
    "        ## TODO 6: apply the second GAT layer\n",
    "        return x\n",
    "\n",
    "# Load the ogbn-arxiv dataset\n",
    "dataset_arxiv = PygNodePropPredDataset(name='ogbn-arxiv')\n",
    "\n",
    "# Let's see some properties of the dataset\n",
    "## TODO 7: print the number of graphs in the dataset (2 points)\n",
    "\n",
    "print(f\"Number of graphs in dataset: {len(dataset_arxiv)}\")\n",
    "\n",
    "## TODO 7: print the number of graphs in the dataset\n",
    "\n",
    "# Let's see how many features and classes are in the dataset\n",
    "## TODO 8: print the number of features and classes in the dataset (2 points)\n",
    "\n",
    "print(f\"Number of features: {dataset_arxiv.num_features}, Number of classes: {dataset_arxiv.num_classes}\")\n",
    "\n",
    "## TODO 8: print the number of features and classes in the dataset\n",
    "\n",
    "\n",
    "data_arxiv = dataset_arxiv[0]\n",
    "# Let's see the shape of the node features and the target labels\n",
    "print(data_arxiv.x.shape)\n",
    "\n",
    "# Get the data split index\n",
    "split_idx_arxiv = dataset_arxiv.get_idx_split()\n",
    "train_idx_arxiv = split_idx_arxiv['train']\n",
    "valid_idx_arxiv = split_idx_arxiv['valid']\n",
    "test_idx_arxiv  = split_idx_arxiv['test']\n",
    "\n",
    "# Model, optimizer, scheduler, and evaluator settings\n",
    "model_arxiv = GAT(in_channels=dataset_arxiv.num_features,\n",
    "                         hidden_channels=64,\n",
    "                         out_channels=dataset_arxiv.num_classes,\n",
    "                         num_heads=8,\n",
    "                         dropout=0.6).to(device)\n",
    "data_arxiv = data_arxiv.to(device)\n",
    "optimizer_arxiv = torch.optim.Adam(model_arxiv.parameters(), lr=0.005, weight_decay=5e-4)\n",
    "scheduler_arxiv = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer_arxiv, mode='min', factor=0.7, patience=10, verbose=True)\n",
    "evaluator_arxiv = Evaluator(name='ogbn-arxiv')\n",
    "\n",
    "def train_arxiv():\n",
    "    model_arxiv.train()\n",
    "    optimizer_arxiv.zero_grad()\n",
    "    ## TODO 9: forward propagation, loss calculation and backward propagation (2 points)\n",
    "    # use F.cross_entropy as the loss function, the input is the output of the model and the target labels, and only use the training set for loss calculation, then call loss.backward()\n",
    "    \n",
    "    out = model_arxiv(data_arxiv)\n",
    "    loss = F.cross_entropy(out[train_idx_arxiv], data_arxiv.y[train_idx_arxiv].squeeze())\n",
    "    loss.backward()\n",
    "\n",
    "    ## TODO 9: forward propagation, loss calculation and backward propagation\n",
    "    optimizer_arxiv.step()\n",
    "    return loss.item()\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_arxiv():\n",
    "    model_arxiv.eval()\n",
    "    ## TODO 10: get the prediction and use argmax to get the final category (2 points)\n",
    "\n",
    "    out = model_arxiv(data_arxiv)\n",
    "    y_pred = out.argmax(dim=1).unsqueeze(1)\n",
    "\n",
    "    ## TODO 10: get the prediction and use argmax to get the final category\n",
    "    train_acc = evaluator_arxiv.eval({'y_true': data_arxiv.y[train_idx_arxiv],\n",
    "                                      'y_pred': y_pred[train_idx_arxiv]})['acc']\n",
    "    valid_acc = evaluator_arxiv.eval({'y_true': data_arxiv.y[valid_idx_arxiv],\n",
    "                                      'y_pred': y_pred[valid_idx_arxiv]})['acc']\n",
    "    test_acc  = evaluator_arxiv.eval({'y_true': data_arxiv.y[test_idx_arxiv],\n",
    "                                      'y_pred': y_pred[test_idx_arxiv]})['acc']\n",
    "    return train_acc, valid_acc, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80a7600734ea1b79",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-10T19:47:13.696601Z",
     "start_time": "2025-03-10T19:47:13.694722Z"
    }
   },
   "outputs": [],
   "source": [
    "## You can change these hyperparameters to see if you can get better results, but the default hyperparameters should work. And also make sure the three sets for hyperparameters are the same.\n",
    "num_epochs = 30\n",
    "best_valid_acc_arxiv = 0\n",
    "patience_arxiv = 30\n",
    "trigger_times_arxiv = 0\n",
    "best_model_state_arxiv = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee08f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import gc\n",
    "\n",
    "# os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "# torch.cuda.empty_cache()\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e896ce1c1aef9bec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-10T19:48:57.548999Z",
     "start_time": "2025-03-10T19:47:13.702337Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- ogbn-arxiv training start ----\n",
      "Epoch: 001, Loss: 6.1625, Train: 0.1791, Valid: 0.0764, Test: 0.0588\n",
      "Epoch: 002, Loss: 5.5791, Train: 0.1312, Valid: 0.2622, Test: 0.2769\n",
      "Epoch: 003, Loss: 5.3758, Train: 0.1888, Valid: 0.2703, Test: 0.2310\n",
      "Epoch: 004, Loss: 5.0632, Train: 0.2032, Valid: 0.2394, Test: 0.2043\n",
      "Epoch: 005, Loss: 4.7448, Train: 0.3108, Valid: 0.2866, Test: 0.2442\n",
      "Epoch: 006, Loss: 4.3284, Train: 0.2624, Valid: 0.2499, Test: 0.2101\n",
      "Epoch: 007, Loss: 4.2126, Train: 0.2613, Valid: 0.2461, Test: 0.2066\n",
      "Epoch: 008, Loss: 4.2357, Train: 0.2906, Valid: 0.2955, Test: 0.2542\n",
      "Epoch: 009, Loss: 4.0538, Train: 0.3509, Valid: 0.3923, Test: 0.3838\n",
      "Epoch: 010, Loss: 3.8687, Train: 0.2964, Valid: 0.3573, Test: 0.3918\n",
      "Epoch: 011, Loss: 3.7791, Train: 0.3802, Valid: 0.4532, Test: 0.4518\n",
      "Epoch: 012, Loss: 3.6805, Train: 0.4049, Valid: 0.4212, Test: 0.3706\n",
      "Epoch: 013, Loss: 3.6393, Train: 0.4142, Valid: 0.4163, Test: 0.3546\n",
      "Epoch: 014, Loss: 3.5714, Train: 0.4137, Valid: 0.4062, Test: 0.3506\n",
      "Epoch: 015, Loss: 3.4898, Train: 0.3830, Valid: 0.3812, Test: 0.3348\n",
      "Epoch: 016, Loss: 3.4022, Train: 0.3575, Valid: 0.3689, Test: 0.3263\n",
      "Epoch: 017, Loss: 3.3475, Train: 0.3544, Valid: 0.3780, Test: 0.3437\n",
      "Epoch: 018, Loss: 3.2906, Train: 0.3844, Valid: 0.4324, Test: 0.4035\n",
      "Epoch: 019, Loss: 3.2483, Train: 0.4078, Valid: 0.4600, Test: 0.4472\n",
      "Epoch: 020, Loss: 3.2036, Train: 0.4284, Valid: 0.4745, Test: 0.4662\n",
      "Epoch: 021, Loss: 3.1645, Train: 0.4558, Valid: 0.4943, Test: 0.4732\n",
      "Epoch: 022, Loss: 3.1152, Train: 0.4653, Valid: 0.4883, Test: 0.4504\n",
      "Epoch: 023, Loss: 3.0915, Train: 0.4676, Valid: 0.4736, Test: 0.4239\n",
      "Epoch: 024, Loss: 3.0500, Train: 0.4740, Valid: 0.4702, Test: 0.4158\n",
      "Epoch: 025, Loss: 3.0178, Train: 0.4818, Valid: 0.4762, Test: 0.4217\n",
      "Epoch: 026, Loss: 2.9770, Train: 0.4847, Valid: 0.4857, Test: 0.4346\n",
      "Epoch: 027, Loss: 2.9570, Train: 0.4811, Valid: 0.4941, Test: 0.4483\n",
      "Epoch: 028, Loss: 2.9245, Train: 0.4770, Valid: 0.5007, Test: 0.4594\n",
      "Epoch: 029, Loss: 2.9105, Train: 0.4772, Valid: 0.5047, Test: 0.4703\n",
      "Epoch: 030, Loss: 2.8707, Train: 0.4813, Valid: 0.5099, Test: 0.4793\n"
     ]
    }
   ],
   "source": [
    "print(\"---- ogbn-arxiv training start ----\")\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    loss = train_arxiv()\n",
    "    scheduler_arxiv.step(loss)\n",
    "    train_acc, valid_acc, test_acc = evaluate_arxiv()\n",
    "    ## TODO 11: early stopping, in an if-else block (2 points)\n",
    "    \n",
    "    if valid_acc > best_valid_acc_arxiv:\n",
    "        best_valid_acc_arxiv = valid_acc\n",
    "        best_model_state_arxiv = model_arxiv.state_dict()\n",
    "        trigger_times_arxiv = 0  # Reset trigger count when improvement is observed\n",
    "    else:\n",
    "        trigger_times_arxiv += 1  # Increment trigger count when no improvement\n",
    "       \n",
    "    ## TODO 11: early stopping, in an if-else block\n",
    "\n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Train: {train_acc:.4f}, Valid: {valid_acc:.4f}, Test: {test_acc:.4f}')\n",
    "\n",
    "    if trigger_times_arxiv >= patience_arxiv:\n",
    "        print(\"Early stopping triggered!\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c271d5a9c3ee72a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-10T19:48:58.449140Z",
     "start_time": "2025-03-10T19:48:57.577423Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ogbn-arxiv] Best validation accuracy: 0.5099, corresponding test accuracy: 0.4793\n"
     ]
    }
   ],
   "source": [
    "# Load the best model state\n",
    "model_arxiv.load_state_dict(best_model_state_arxiv)\n",
    "final_train, final_valid, final_test = evaluate_arxiv()\n",
    "print(f\"[ogbn-arxiv] Best validation accuracy: {final_valid:.4f}, corresponding test accuracy: {final_test:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473e14fc034a8236",
   "metadata": {},
   "source": [
    "* What's the best validation accuracy you can get? (2 points) What's the corresponding test accuracy? (2 points) Please report the results in this markdown cell.\n",
    "<br><br>**ANS:**\n",
    "Best Validation Accuracy: 0.5099\n",
    "Corresponding Test Accuracy: 0.4793\n",
    "These are the best results from the ogbn-arxiv dataset after training Graph Attention Network (GAT) model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861c6925420663ae",
   "metadata": {},
   "source": [
    "* Let's see if we remove the graph structure and only use the node features, how well the model can perform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8510bc73d5baef22",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-10T19:49:34.189297Z",
     "start_time": "2025-03-10T19:48:58.464181Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 7.0533, Train: 0.1792, Valid: 0.0766, Test: 0.0587\n",
      "Epoch: 002, Loss: 6.5416, Train: 0.1429, Valid: 0.2423, Test: 0.2257\n",
      "Epoch: 003, Loss: 5.9118, Train: 0.2650, Valid: 0.3299, Test: 0.3116\n",
      "Epoch: 004, Loss: 5.6113, Train: 0.3427, Valid: 0.3552, Test: 0.3180\n",
      "Epoch: 005, Loss: 5.2832, Train: 0.3241, Valid: 0.3148, Test: 0.2801\n",
      "Epoch: 006, Loss: 5.0222, Train: 0.2780, Valid: 0.2545, Test: 0.2253\n",
      "Epoch: 007, Loss: 4.8658, Train: 0.2747, Valid: 0.2611, Test: 0.2304\n",
      "Epoch: 008, Loss: 4.7486, Train: 0.3013, Valid: 0.3079, Test: 0.2811\n",
      "Epoch: 009, Loss: 4.5691, Train: 0.3454, Valid: 0.3669, Test: 0.3491\n",
      "Epoch: 010, Loss: 4.4079, Train: 0.3682, Valid: 0.3999, Test: 0.3833\n",
      "Epoch: 011, Loss: 4.3124, Train: 0.3751, Valid: 0.4112, Test: 0.3891\n",
      "Epoch: 012, Loss: 4.2166, Train: 0.4078, Valid: 0.4349, Test: 0.4084\n",
      "Epoch: 013, Loss: 4.0907, Train: 0.4302, Valid: 0.4444, Test: 0.4163\n",
      "Epoch: 014, Loss: 3.9909, Train: 0.4111, Valid: 0.4213, Test: 0.3970\n",
      "Epoch: 015, Loss: 3.9094, Train: 0.3887, Valid: 0.4001, Test: 0.3781\n",
      "Epoch: 016, Loss: 3.8234, Train: 0.3773, Valid: 0.3933, Test: 0.3726\n",
      "Epoch: 017, Loss: 3.7562, Train: 0.3829, Valid: 0.4006, Test: 0.3828\n",
      "Epoch: 018, Loss: 3.6879, Train: 0.3966, Valid: 0.4136, Test: 0.3979\n",
      "Epoch: 019, Loss: 3.6145, Train: 0.4110, Valid: 0.4290, Test: 0.4108\n",
      "Epoch: 020, Loss: 3.5467, Train: 0.4190, Valid: 0.4376, Test: 0.4144\n",
      "Epoch: 021, Loss: 3.5015, Train: 0.4250, Valid: 0.4478, Test: 0.4215\n",
      "Epoch: 022, Loss: 3.4631, Train: 0.4341, Valid: 0.4543, Test: 0.4316\n",
      "Epoch: 023, Loss: 3.4136, Train: 0.4375, Valid: 0.4564, Test: 0.4362\n",
      "Epoch: 024, Loss: 3.3756, Train: 0.4272, Valid: 0.4441, Test: 0.4258\n",
      "Epoch: 025, Loss: 3.3442, Train: 0.4141, Valid: 0.4303, Test: 0.4127\n",
      "Epoch: 026, Loss: 3.3188, Train: 0.4063, Valid: 0.4241, Test: 0.4072\n",
      "Epoch: 027, Loss: 3.2970, Train: 0.4044, Valid: 0.4218, Test: 0.4025\n",
      "Epoch: 028, Loss: 3.2763, Train: 0.4090, Valid: 0.4260, Test: 0.4061\n",
      "Epoch: 029, Loss: 3.2691, Train: 0.4182, Valid: 0.4363, Test: 0.4164\n",
      "Epoch: 030, Loss: 3.2466, Train: 0.4219, Valid: 0.4428, Test: 0.4226\n",
      "[ogbn-arxiv_no_graph] Best validation accuracy: 0.4428, corresponding test accuracy: 0.4226\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "# construct a dummy edge_index with self-loops only\n",
    "num_nodes = data_arxiv.num_nodes\n",
    "dummy_edge_index = torch.arange(num_nodes, device=data_arxiv.x.device).unsqueeze(0).repeat(2, 1)\n",
    "\n",
    "# copy the original data and replace the edge_index with the dummy one\n",
    "data_arxiv_no_graph = copy.deepcopy(data_arxiv)\n",
    "data_arxiv_no_graph.edge_index = dummy_edge_index\n",
    "\n",
    "# define a new model for the data without graph structure, with the same hyperparameters\n",
    "model_no_graph = GAT(\n",
    "    in_channels=dataset_arxiv.num_features,\n",
    "    hidden_channels=64,\n",
    "    out_channels=dataset_arxiv.num_classes,\n",
    "    num_heads=8,\n",
    "    dropout=0.6\n",
    ").to(device)\n",
    "\n",
    "\n",
    "optimizer_no_graph = torch.optim.Adam(model_no_graph.parameters(), lr=0.005, weight_decay=5e-4)\n",
    "scheduler_no_graph = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer_no_graph, mode='min', factor=0.7, patience=10, verbose=True\n",
    ")\n",
    "\n",
    "def train_no_graph():\n",
    "    ## TODO 12: training code for the model without graph structure, should be the same as the original training code (2 points)\n",
    "\n",
    "    # use the same model but with data without graph structure\n",
    "    model_no_graph.train()\n",
    "    optimizer_no_graph.zero_grad()\n",
    "\n",
    "    # Forward pass with data without the graph structure\n",
    "    out = model_no_graph(data_arxiv_no_graph)\n",
    "    \n",
    "    # Compute loss\n",
    "    loss = F.cross_entropy(out[train_idx_arxiv], data_arxiv_no_graph.y[train_idx_arxiv].squeeze())\n",
    "    \n",
    "    # Backward pass and optimization\n",
    "    loss.backward()\n",
    "    optimizer_no_graph.step()\n",
    "\n",
    "    ## TODO 12: training code for the model without graph structure, should be the same as the original training code\n",
    "    return loss.item()\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_no_graph():\n",
    "    ## TODO 13: evaluation code for the model without graph structure, should be the same as the original evaluation code (2 points)\n",
    "\n",
    "    model_no_graph.eval()\n",
    "    \n",
    "    # Forward pass with data without the graph structure\n",
    "    out = model_no_graph(data_arxiv_no_graph)\n",
    "    y_pred = out.argmax(dim=1).unsqueeze(1) \n",
    "    \n",
    "    # Calculate accuracy for train, valid, and test sets\n",
    "    train_acc = evaluator_arxiv.eval({'y_true': data_arxiv_no_graph.y[train_idx_arxiv], 'y_pred': y_pred[train_idx_arxiv]})['acc']\n",
    "    valid_acc = evaluator_arxiv.eval({'y_true': data_arxiv_no_graph.y[valid_idx_arxiv], 'y_pred': y_pred[valid_idx_arxiv]})['acc']\n",
    "    test_acc  = evaluator_arxiv.eval({'y_true': data_arxiv_no_graph.y[test_idx_arxiv], 'y_pred': y_pred[test_idx_arxiv]})['acc']\n",
    "\n",
    "    ## TODO 13: evaluation code for the model without graph structure, should be the same as the original evaluation code\n",
    "    return train_acc, valid_acc, test_acc\n",
    "\n",
    "\n",
    "## You can change these hyperparameters to see if you can get better results, but the default hyperparameters should work. And also make sure the three sets for hyperparameters are the same.\n",
    "num_epochs_no_graph = 30\n",
    "best_valid_acc_arxiv_no_graph = 0\n",
    "patience_arxiv_no_graph = 30\n",
    "trigger_times_arxiv_no_graph = 0\n",
    "best_model_state_arxiv_no_graph = None\n",
    "# start training and evaluation\n",
    "for epoch in range(1, num_epochs_no_graph+1):\n",
    "    loss = train_no_graph()\n",
    "    scheduler_no_graph.step(loss)\n",
    "    train_acc, valid_acc, test_acc = evaluate_no_graph()\n",
    "    ## TODO 14: early stopping, in an if-else block (2 points)\n",
    "\n",
    "    if valid_acc > best_valid_acc_arxiv_no_graph:\n",
    "        best_valid_acc_arxiv_no_graph = valid_acc\n",
    "        best_model_state_arxiv_no_graph = model_no_graph.state_dict()\n",
    "        trigger_times_arxiv_no_graph = 0  # Reset trigger count when improvement is observed\n",
    "    else:\n",
    "        trigger_times_arxiv_no_graph += 1  # Increment trigger count when no improvement\n",
    "        \n",
    "    ## TODO 14: early stopping, in an if-else block\n",
    "\n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Train: {train_acc:.4f}, Valid: {valid_acc:.4f}, Test: {test_acc:.4f}')\n",
    "\n",
    "    if trigger_times_arxiv_no_graph >= patience_arxiv_no_graph:\n",
    "        print(\"Early stopping triggered!\")\n",
    "        break\n",
    "# Load the best model state\n",
    "model_no_graph.load_state_dict(best_model_state_arxiv_no_graph)\n",
    "final_train, final_valid, final_test = evaluate_no_graph()\n",
    "print(f\"[ogbn-arxiv_no_graph] Best validation accuracy: {final_valid:.4f}, corresponding test accuracy: {final_test:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e2af5f42b0b57d",
   "metadata": {},
   "source": [
    "* What's the best validation accuracy you can get? (2 points) What's the corresponding test accuracy? (2 points) Please report the results in this markdown cell.\n",
    "<br><br>**ANS:**\n",
    "Best Validation Accuracy: 0.4428\n",
    "Corresponding Test Accuracy: 0.4226"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574446b1e446f890",
   "metadata": {},
   "source": [
    "* Let's see if we further remove the edge structure and only use the node features, how well the model can perform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99a3de7c982f410",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-10T19:50:08.093156Z",
     "start_time": "2025-03-10T19:49:34.212367Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 6.9605, Train: 0.1791, Valid: 0.0763, Test: 0.0586\n",
      "Epoch: 002, Loss: 6.5561, Train: 0.1176, Valid: 0.2347, Test: 0.2204\n",
      "Epoch: 003, Loss: 6.1048, Train: 0.1989, Valid: 0.2981, Test: 0.3183\n",
      "Epoch: 004, Loss: 5.7291, Train: 0.2972, Valid: 0.2876, Test: 0.2595\n",
      "Epoch: 005, Loss: 5.3743, Train: 0.3102, Valid: 0.2910, Test: 0.2552\n",
      "Epoch: 006, Loss: 5.0659, Train: 0.2447, Valid: 0.2001, Test: 0.1749\n",
      "Epoch: 007, Loss: 4.9449, Train: 0.2503, Valid: 0.2198, Test: 0.1909\n",
      "Epoch: 008, Loss: 4.8078, Train: 0.2863, Valid: 0.2869, Test: 0.2582\n",
      "Epoch: 009, Loss: 4.6061, Train: 0.3318, Valid: 0.3591, Test: 0.3436\n",
      "Epoch: 010, Loss: 4.4422, Train: 0.3622, Valid: 0.4134, Test: 0.4135\n",
      "Epoch: 011, Loss: 4.3661, Train: 0.3767, Valid: 0.4285, Test: 0.4242\n",
      "Epoch: 012, Loss: 4.2621, Train: 0.3887, Valid: 0.4230, Test: 0.3997\n",
      "Epoch: 013, Loss: 4.1627, Train: 0.4141, Valid: 0.4259, Test: 0.3943\n",
      "Epoch: 014, Loss: 4.0409, Train: 0.4204, Valid: 0.4268, Test: 0.3930\n",
      "Epoch: 015, Loss: 3.9257, Train: 0.3974, Valid: 0.4051, Test: 0.3760\n",
      "Epoch: 016, Loss: 3.8439, Train: 0.3753, Valid: 0.3873, Test: 0.3608\n",
      "Epoch: 017, Loss: 3.7707, Train: 0.3720, Valid: 0.3887, Test: 0.3648\n",
      "Epoch: 018, Loss: 3.7021, Train: 0.3854, Valid: 0.4095, Test: 0.3935\n",
      "Epoch: 019, Loss: 3.6369, Train: 0.4051, Valid: 0.4354, Test: 0.4225\n",
      "Epoch: 020, Loss: 3.5698, Train: 0.4155, Valid: 0.4455, Test: 0.4303\n",
      "Epoch: 021, Loss: 3.5164, Train: 0.4133, Valid: 0.4356, Test: 0.4120\n",
      "Epoch: 022, Loss: 3.4704, Train: 0.4152, Valid: 0.4310, Test: 0.3995\n",
      "Epoch: 023, Loss: 3.4301, Train: 0.4266, Valid: 0.4381, Test: 0.4078\n",
      "Epoch: 024, Loss: 3.3967, Train: 0.4317, Valid: 0.4440, Test: 0.4161\n",
      "Epoch: 025, Loss: 3.3576, Train: 0.4272, Valid: 0.4419, Test: 0.4163\n",
      "Epoch: 026, Loss: 3.3292, Train: 0.4165, Valid: 0.4358, Test: 0.4143\n",
      "Epoch: 027, Loss: 3.3053, Train: 0.4080, Valid: 0.4315, Test: 0.4126\n",
      "Epoch: 028, Loss: 3.2981, Train: 0.4054, Valid: 0.4308, Test: 0.4138\n",
      "Epoch: 029, Loss: 3.2758, Train: 0.4083, Valid: 0.4329, Test: 0.4152\n",
      "Epoch: 030, Loss: 3.2628, Train: 0.4134, Valid: 0.4354, Test: 0.4122\n",
      "[ogbn-arxiv_no_edge] Best validation accuracy: 0.4354, corresponding test accuracy: 0.4122\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "# an empty edge_index, even no self-loops\n",
    "dummy_edge_index = torch.empty((2, 0), dtype=torch.long, device=data_arxiv.x.device)\n",
    "\n",
    "# duplicate the original data and replace the edge_index with the dummy one\n",
    "data_arxiv_no_edge = copy.deepcopy(data_arxiv)\n",
    "data_arxiv_no_edge.edge_index = dummy_edge_index\n",
    "\n",
    "# define a new model for the data without graph structure, with the same hyperparameters\n",
    "model_no_edge = GAT(\n",
    "    in_channels=dataset_arxiv.num_features,\n",
    "    hidden_channels=64,\n",
    "    out_channels=dataset_arxiv.num_classes,\n",
    "    num_heads=8,\n",
    "    dropout=0.6\n",
    ").to(device)\n",
    "\n",
    "optimizer_no_edge = torch.optim.Adam(model_no_edge.parameters(), lr=0.005, weight_decay=5e-4)\n",
    "scheduler_no_edge = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer_no_edge, mode='min', factor=0.7, patience=10, verbose=True\n",
    ")\n",
    "\n",
    "def train_no_edge():\n",
    "    ## TODO 15: training code for the model without edge structure, should be the same as the original training code (2 points)\n",
    "\n",
    "    # use the same model but with data without graph structure\n",
    "    model_no_edge.train()\n",
    "    optimizer_no_edge.zero_grad()\n",
    "\n",
    "    # Forward pass with data without edge structure\n",
    "    out = model_no_edge(data_arxiv_no_edge)\n",
    "    \n",
    "    # Compute the loss\n",
    "    loss = F.cross_entropy(out[train_idx_arxiv], data_arxiv_no_edge.y[train_idx_arxiv].squeeze())\n",
    "    \n",
    "    # Backward pass and optimization\n",
    "    loss.backward()\n",
    "    optimizer_no_edge.step()\n",
    "\n",
    "    ## TODO 15: training code for the model without edge structure, should be the same as the original training code\n",
    "    return loss.item()\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_no_edge():\n",
    "    ## TODO 16: evaluation code for the model without edge structure, should be the same as the original evaluation code (2 points)\n",
    "\n",
    "    model_no_edge.eval()\n",
    "\n",
    "    # Forward pass with data without edge structure\n",
    "    out = model_no_edge(data_arxiv_no_edge)\n",
    "    y_pred = out.argmax(dim=1).unsqueeze(1)  # Reshape y_pred to match the shape of y_true\n",
    "\n",
    "    # Calculate accuracy for train, valid, and test sets\n",
    "    train_acc = evaluator_arxiv.eval({'y_true': data_arxiv_no_edge.y[train_idx_arxiv], 'y_pred': y_pred[train_idx_arxiv]})['acc']\n",
    "    valid_acc = evaluator_arxiv.eval({'y_true': data_arxiv_no_edge.y[valid_idx_arxiv], 'y_pred': y_pred[valid_idx_arxiv]})['acc']\n",
    "    test_acc  = evaluator_arxiv.eval({'y_true': data_arxiv_no_edge.y[test_idx_arxiv], 'y_pred': y_pred[test_idx_arxiv]})['acc']\n",
    "\n",
    "    ## TODO 16: evaluation code for the model without edge structure, should be the same as the original evaluation code\n",
    "    return train_acc, valid_acc, test_acc\n",
    "\n",
    "\n",
    "## You can change these hyperparameters to see if you can get better results, but the default hyperparameters should work. And also make sure the three sets for hyperparameters are the same.\n",
    "num_epochs_no_edge = 30\n",
    "best_valid_acc_no_edge = 0\n",
    "patience_no_edge = 30\n",
    "trigger_times_no_edge = 0\n",
    "best_model_state_no_edge = None\n",
    "\n",
    "# start training and evaluation\n",
    "for epoch in range(1, num_epochs_no_edge+1):\n",
    "    loss = train_no_edge()\n",
    "    scheduler_no_edge.step(loss)\n",
    "    train_acc, valid_acc, test_acc = evaluate_no_edge()\n",
    "    ## TODO 17: early stopping, in an if-else block (2 points)\n",
    "\n",
    "    if valid_acc > best_valid_acc_no_edge:\n",
    "        best_valid_acc_no_edge = valid_acc\n",
    "        best_model_state_no_edge = model_no_edge.state_dict()\n",
    "        trigger_times_no_edge = 0  # Reset trigger count when improvement is observed\n",
    "    else:\n",
    "        trigger_times_no_edge += 1  # Increment trigger count when no improvement\n",
    "\n",
    "    ## TODO 17: early stopping, in an if-else block\n",
    "\n",
    "    print(f\"Epoch: {epoch:03d}, Loss: {loss:.4f}, Train: {train_acc:.4f}, Valid: {valid_acc:.4f}, Test: {test_acc:.4f}\")\n",
    "\n",
    "    if trigger_times_no_edge >= patience_no_edge:\n",
    "        print(\"Early stopping triggered!\")\n",
    "        break\n",
    "\n",
    "# load the best model state\n",
    "model_no_edge.load_state_dict(best_model_state_no_edge)\n",
    "final_train, final_valid, final_test = evaluate_no_edge()\n",
    "print(f\"[ogbn-arxiv_no_edge] Best validation accuracy: {final_valid:.4f}, corresponding test accuracy: {final_test:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c182c6357029e6",
   "metadata": {},
   "source": [
    "* What's the best validation accuracy you can get? What's the corresponding test accuracy? Please report the results in this markdown cell (2 points).\n",
    "<br><br>**ANS:**\n",
    "Best Validation Accuracy: 0.4354\n",
    "Corresponding Test Accuracy: 0.4122<br><br>\n",
    "* What's your observation of the results (all the three situations)? Please write down your observation in this markdown cell (2 points).\n",
    "<br><br>**ANS:**<br>\n",
    "\n",
    "**Observation of the Results:**\n",
    "\n",
    "**Full Graph Structure (GAT model):**\n",
    "\n",
    "The best performance is achieved when the model utilizes both node features and graph edges. The GAT model captures the relationships between nodes effectively, leading to the highest validation and test accuracy.\n",
    "\n",
    "**No Graph Structure (Only Node Features):**\n",
    "\n",
    "When the graph structure is removed and the model only has access to node features, performance declines significantly. The model loses the ability to capture inter-node relationships, resulting in a noticeable drop in accuracy on both validation and test sets.\n",
    "\n",
    "**No Edge Structure (Self-Loops Only):**\n",
    "\n",
    "When the edge structure is further removed, leaving only self-loops (i.e., each node is connected only to itself), the performance drops even further. The minimal structure provided by self-loops doesn't offer enough information for the model to make accurate predictions, causing the test accuracy to be even lower than the model using only node features.\n",
    "\n",
    "**Summary:**\n",
    "\n",
    "The graph structure (edges) plays a crucial role in node classification tasks, as it enables the model to learn relationships between nodes. As we progressively remove the graph structure (first removing all edges, then using only self-loops), the model's performance declines. This highlights how important the topology of the graph is for capturing node dependencies, which is key for accurate predictions in graph-based tasks.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df05ed3c",
   "metadata": {},
   "source": [
    "## Graph Convolutional Network (GCN) Implementation for Graph Classification\n",
    "\n",
    "**Objective:** Implement a GCN from scratch to perform graph classification using an OGB dataset. Develop neural components, including the forward pass, as well as training and testing routines. There are 11 `todo`s and 7 questions for GCN.\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "Provide an overview of Graph Neural Networks (GNNs) and the significance of GCNs in processing graph-structured data. Discuss the relevance of graph classification tasks and the role of the OGB datasets in benchmarking.\n",
    "\n",
    "## 2. Dataset Exploration\n",
    "\n",
    "### 2.1. Importing Libraries\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b75bc96aa0e8457",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-10T19:50:08.142940Z",
     "start_time": "2025-03-10T19:50:08.096087Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import DataLoader\n",
    "from ogb.graphproppred import PygGraphPropPredDataset, Evaluator\n",
    "import torch_geometric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1159e827",
   "metadata": {},
   "source": [
    "### 2.2. Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "46b5c13357b23e0d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-10T19:50:08.199196Z",
     "start_time": "2025-03-10T19:50:08.166681Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\monat\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\ogb\\graphproppred\\dataset_pyg.py:68: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.data, self.slices = torch.load(self.processed_paths[0])\n",
      "c:\\Users\\monat\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch_geometric\\deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    }
   ],
   "source": [
    "# Import necessary modules\n",
    "from ogb.graphproppred import PygGraphPropPredDataset  # OGB dataset loader for graph property prediction\n",
    "from torch_geometric.data import DataLoader  # PyG DataLoader for handling batches of graphs\n",
    "\n",
    "# Load the OGB dataset for molecular property prediction.\n",
    "# 'ogbg-molhiv' is a graph-level dataset where each graph represents a molecular structure,\n",
    "# and the task is binary classification to predict whether the molecule inhibits HIV replication.\n",
    "dataset = PygGraphPropPredDataset(name='ogbg-molhiv')\n",
    "\n",
    "# Split the dataset into training, validation, and test sets.\n",
    "# The dataset provides predefined splits to ensure consistency in evaluation.\n",
    "split_idx = dataset.get_idx_split()\n",
    "train_dataset = dataset[split_idx['train']]  # Training set\n",
    "valid_dataset = dataset[split_idx['valid']]  # Validation set\n",
    "test_dataset = dataset[split_idx['test']]    # Test set\n",
    "\n",
    "# Create DataLoaders for batch processing.\n",
    "# The DataLoader enables efficient loading of graphs in batches for training and evaluation.\n",
    "# - batch_size: Number of graphs per batch.\n",
    "# - shuffle: Whether to shuffle the dataset at each epoch (only done for training).\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)   # Shuffle for training\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=128, shuffle=False)  # No shuffle for validation\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)    # No shuffle for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c5b9c5767405fc08",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-10T19:50:08.226366Z",
     "start_time": "2025-03-10T19:50:08.223587Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset name: ogbg-molhiv\n",
      "Number of graphs: 41127\n",
      "Number of classes: 1\n",
      "Number of node features: 9\n"
     ]
    }
   ],
   "source": [
    "# Display dataset information\n",
    "print(f'Dataset name: {dataset.name}')\n",
    "print(f'Number of graphs: {len(dataset)}')\n",
    "print(f'Number of classes: {dataset.num_tasks}')\n",
    "print(f'Number of node features: {dataset.num_node_features}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0999f3e",
   "metadata": {},
   "source": [
    "## 3. Model Implementation\n",
    "### 3.1. GCN Convolution Layer Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "de2590fac4e0f3e0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-10T19:50:08.253831Z",
     "start_time": "2025-03-10T19:50:08.250111Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch_geometric.nn import MessagePassing  # Base class for defining message-passing layers\n",
    "from torch_geometric.utils import add_self_loops, degree  # Utilities for graph processing\n",
    "\n",
    "class GCNConv(MessagePassing):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        \"\"\"\n",
    "        Custom implementation of a Graph Convolutional Network (GCN) layer.\n",
    "\n",
    "        Args:\n",
    "            in_channels (int): Number of input node features.\n",
    "            out_channels (int): Number of output node features.\n",
    "\n",
    "        The layer follows the formulation:\n",
    "            H' = σ(D^(-1/2) A D^(-1/2) H W)\n",
    "        where:\n",
    "            - A is the adjacency matrix with self-loops.\n",
    "            - D is the degree matrix.\n",
    "            - H is the input node feature matrix.\n",
    "            - W is the trainable weight matrix.\n",
    "            - σ is a non-linearity (like ReLU).\n",
    "        \"\"\"\n",
    "        super(GCNConv, self).__init__(aggr='add')  # \"Add\" aggregation means summing messages from neighbors.\n",
    "\n",
    "        # TODO 1: Define a linear transformation layer for node features (2 points)\n",
    "\n",
    "        self.linear = nn.Linear(in_channels, out_channels)\n",
    "\n",
    "        # TODO 1: Define a linear transformation layer for node features\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        \"\"\"\n",
    "        Forward pass of the GCN layer.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): Node feature matrix of shape [num_nodes, in_channels].\n",
    "            edge_index (Tensor): Graph connectivity in COO format, shape [2, num_edges].\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Updated node features of shape [num_nodes, out_channels].\n",
    "        \"\"\"\n",
    "\n",
    "        # TODO 2: Add self-loops to the adjacency matrix (2 points)\n",
    "\n",
    "        edge_index, _ = add_self_loops(edge_index, num_nodes=x.size(0))\n",
    "\n",
    "        # TODO 2: Add self-loops to the adjacency matrix\n",
    "\n",
    "        # TODO 3: Compute node degrees (2 points)\n",
    "\n",
    "        row, col = edge_index  # Extract row (source nodes) and col (destination nodes)\n",
    "        deg = degree(row, x.size(0), dtype=x.dtype)  # Compute degree of each node\n",
    "        deg_inv_sqrt = deg.pow(-0.5)  # Compute D^(-1/2)\n",
    "\n",
    "        # TODO 3: Compute node degrees \n",
    "\n",
    "        # Prevent division by zero for isolated nodes\n",
    "        deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0  \n",
    "\n",
    "        # TODO 4: Compute normalized adjacency matrix (2 points)\n",
    "\n",
    "        norm = deg_inv_sqrt[row] * deg_inv_sqrt[col]  # Normalize using degrees\n",
    "\n",
    "        # TODO 4: Compute normalized adjacency matrix \n",
    "\n",
    "        # TODO 5: Perform message passing using PyTorch Geometric's propagate function (2 points)\n",
    "\n",
    "        out = self.propagate(edge_index, x=x, norm=norm)\n",
    "\n",
    "        # TODO 5: Perform message passing using PyTorch Geometric's propagate function \n",
    "\n",
    "        # TODO 6: Apply the linear transformation after aggregation (2 points)\n",
    "\n",
    "        out = self.linear(out)\n",
    "\n",
    "        # TODO 6: Apply the linear transformation after aggregation \n",
    "\n",
    "        return out\n",
    "\n",
    "    def message(self, x_j, norm):\n",
    "        \"\"\"\n",
    "        Message function: Defines how information is aggregated from neighboring nodes.\n",
    "\n",
    "        Args:\n",
    "            x_j (Tensor): Features of neighboring nodes.\n",
    "            norm (Tensor): Normalization coefficients computed from node degrees.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Normalized node features.\n",
    "        \"\"\"\n",
    "        # TODO 7: Apply normalization to the node features (2 points)\n",
    "\n",
    "        return norm.view(-1, 1) * x_j \n",
    "    \n",
    "        # TODO 7: Apply normalization to the node features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb87f36",
   "metadata": {},
   "source": [
    "## TODO:\n",
    "### Why is it important to add self-loops in the graph convolution process, and how does the normalization strategy here help stabilize training in graph neural networks? (2 points)\n",
    "\n",
    "**ANS:**\n",
    "\n",
    "**Importance of Adding Self-Loops:**\n",
    "\n",
    "Self-loops are crucial in graph convolutions as they allow each node to retain its own information during the aggregation process. Without self-loops, nodes could lose their identity across layers, leading to ineffective learning.\n",
    "\n",
    "**Normalization Strategy for Stabilizing Training:**\n",
    "\n",
    "The normalization strategy, using the inverse square root of node degrees, helps stabilize training by preventing nodes with high degrees from dominating the message aggregation. This ensures balanced information flow, avoids exploding gradients, and promotes stable learning in deep networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd50c82",
   "metadata": {},
   "source": [
    "### 3.2. Implement GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "95f2f3f44c367f1a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-10T19:50:08.280885Z",
     "start_time": "2025-03-10T19:50:08.278130Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, num_node_features, hidden_channels, num_classes, dropout_rate):\n",
    "        \"\"\"\n",
    "        Graph Convolutional Network (GCN) for graph classification.\n",
    "\n",
    "        Args:\n",
    "            num_node_features (int): Number of input node features.\n",
    "            hidden_channels (int): Number of hidden units in the GCN layers.\n",
    "            num_classes (int): Number of output classes (for classification tasks).\n",
    "            dropout_rate (float): Dropout probability for regularization.\n",
    "\n",
    "        This model consists of:\n",
    "        - Two GCNConv layers to extract graph structure-aware features.\n",
    "        - ReLU activation and dropout for regularization.\n",
    "        - A global mean pooling layer to aggregate node features into graph-level representations.\n",
    "        - A final linear layer for classification.\n",
    "        \"\"\"\n",
    "        super(GCN, self).__init__()\n",
    "\n",
    "        self.dropout_rate = dropout_rate  # Store dropout rate\n",
    "\n",
    "        # TODO 8: Complete the network layer definitions (2 points)\n",
    "\n",
    "        # First graph convolution layer: transforms node features to hidden dimension\n",
    "        self.gcn1 = GCNConv(num_node_features, hidden_channels)\n",
    "\n",
    "        # Second graph convolution layer: refines node embeddings\n",
    "        self.gcn2 = GCNConv(hidden_channels, hidden_channels)\n",
    "\n",
    "        # Final linear layer to produce class predictions from the pooled graph representation\n",
    "        self.fc = nn.Linear(hidden_channels, num_classes)\n",
    "\n",
    "        # TODO 8: Complete the network layer definitions (2 points)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        \"\"\"\n",
    "        Forward pass through the GCN.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): Node feature matrix of shape [num_nodes, num_node_features].\n",
    "            edge_index (Tensor): Graph connectivity in COO format [2, num_edges].\n",
    "            batch (Tensor): Batch index for each node, used for global pooling.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Output class logits of shape [num_graphs, num_classes].\n",
    "        \"\"\"\n",
    "\n",
    "        # TODO 9: Complete the implementation of the forward function (2 points)\n",
    "\n",
    "        # First GCN layer: apply convolution, activation, and dropout\n",
    "        x = self.gcn1(x, edge_index)\n",
    "        x = F.relu(x)  # ReLU activation layer\n",
    "        x = F.dropout(x, p=self.dropout_rate, training=self.training)  # Dropout layer for regularization\n",
    "\n",
    "        # Second GCN layer\n",
    "        x = self.gcn2(x, edge_index)\n",
    "        x = F.relu(x)  # ReLU activation layer\n",
    "        x = F.dropout(x, p=self.dropout_rate, training=self.training)  # Dropout layer for regularization\n",
    "\n",
    "        # Global mean pooling to aggregate node features into a graph representation\n",
    "        x = global_mean_pool(x, batch)  # Output shape: [num_graphs, hidden_channels]\n",
    "\n",
    "        # Final linear layer for classification\n",
    "        x = self.fc(x)  # Output shape: [num_graphs, num_classes]\n",
    "\n",
    "        # TODO 9: Complete the implementation of the forward function (2 points)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f28461",
   "metadata": {},
   "source": [
    "## 4. Model Training\n",
    "### 4.1. Implement Training and Evalutation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "877b38046824a623",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-10T19:50:08.306599Z",
     "start_time": "2025-03-10T19:50:08.304235Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(model, loader, device):\n",
    "    \"\"\"\n",
    "    Training function for the GCN model.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The GCN model.\n",
    "        loader (DataLoader): DataLoader for the training set.\n",
    "        criterion (torch.nn.Module): Loss function.\n",
    "        optimizer (torch.optim.Optimizer): Optimizer for training.\n",
    "        device (torch.device): Device to run computations on (CPU or GPU).\n",
    "\n",
    "    Returns:\n",
    "        float: Average training loss per graph.\n",
    "    \"\"\"\n",
    "\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    # Iterate over batches in the DataLoader\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "\n",
    "        # TODO 10: Complete the training function (2 points)\n",
    "\n",
    "        #  Zero the gradients of the Adam optimizer from `torch.optim`\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Perform a forward pass through the model\n",
    "        out = model(data.x, data.edge_index, data.batch)\n",
    "\n",
    "        # Compute the loss use `BCEWithLogitsLoss()``\n",
    "\n",
    "        # This loss is suitable for binary classification tasks (e.g., graph classification)\n",
    "        loss = F.binary_cross_entropy_with_logits(out, data.y.float())\n",
    "\n",
    "        #  Perform backpropagation\n",
    "        loss.backward()\n",
    "\n",
    "        # Update model parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # TODO 10: Complete the training function\n",
    "\n",
    "        # Accumulate loss\n",
    "        total_loss += loss.item() * data.num_graphs\n",
    "\n",
    "    return total_loss / len(loader.dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e375aab5",
   "metadata": {},
   "source": [
    "## TODO:\n",
    "### How does the choice of the BCEWithLogitsLoss and the Adam optimizer influence the training dynamics in this setting? (2 points)\n",
    "\n",
    "**ANS:**\n",
    "\n",
    "1. **BCEWithLogitsLoss**:\n",
    "   - **BCEWithLogitsLoss** is a loss function specifically designed for binary classification tasks, where the outputs are logits (raw, unnormalized scores) rather than probabilities. It combines a **sigmoid activation** with **binary cross-entropy loss**, which makes it numerically stable by avoiding potential issues with small values from separate sigmoid and loss operations.\n",
    "   - **Effect on Training**: Since the GCN model produces logits (not probabilities), BCEWithLogitsLoss directly compares the raw logits with the target labels. This is ideal for graph-based binary classification tasks, as it allows the model to learn and update its parameters efficiently while avoiding computational issues related to individual sigmoid and cross-entropy steps.\n",
    "\n",
    "2. **Adam Optimizer**:\n",
    "   - **Adam** (short for Adaptive Moment Estimation) is an adaptive learning rate optimization algorithm that computes individual learning rates for different parameters based on estimates of first and second moments (mean and uncentered variance). It combines the benefits of **Momentum** and **RMSprop**, making it well-suited for training deep learning models.\n",
    "   - **Effect on Training**: Adam adapts the learning rate for each parameter, which helps speed up training and provides robustness against poor initialization. This is particularly useful in graph-based models where the structure of the data can vary significantly. Adam's ability to adapt to the landscape of the loss function helps the model converge faster and more reliably, especially when dealing with complex graph structures.\n",
    "\n",
    "\n",
    "### What are potential alternative choices and their pros/cons for graph-based binary classification tasks? (2 points)\n",
    "\n",
    "**ANS:**\n",
    "1. **Alternative Loss Functions**:\n",
    "   \n",
    "   - **CrossEntropyLoss (for multi-class classification)**:\n",
    "     - **Pros**: Useful if the binary classification task is extended to multi-class problems, as it can handle multi-class classification directly.\n",
    "     - **Cons**: If the task is strictly binary, BCEWithLogitsLoss would be more appropriate since CrossEntropyLoss requires one-hot encoded labels and can be less efficient for binary tasks.\n",
    "   \n",
    "   - **Hinge Loss (SVM-based)**:\n",
    "     - **Pros**: Effective for margin-based classification tasks where you care about not just correct classification but also the confidence in classification. It can lead to better generalization.\n",
    "     - **Cons**: Not as commonly used in graph neural networks, especially for tasks like graph classification, where BCEWithLogitsLoss typically provides a smoother and more efficient training process.\n",
    "\n",
    "2. **Alternative Optimizers**:\n",
    "   \n",
    "   - **Stochastic Gradient Descent (SGD)**:\n",
    "     - **Pros**: One of the simplest and most widely used optimizers, suitable for large-scale datasets and relatively simple models. It is a good option if computational resources are limited.\n",
    "     - **Cons**: Does not adapt learning rates, making it slower to converge in deep learning tasks. Requires careful tuning of the learning rate and momentum parameters, which can be cumbersome.\n",
    "\n",
    "   - **RMSprop**:\n",
    "     - **Pros**: Like Adam, it adapts the learning rate based on recent gradient information, but it does so by maintaining a moving average of squared gradients. Works well with non-stationary objectives.\n",
    "     - **Cons**: While it adapts learning rates like Adam, it can be less robust for certain tasks and can require tuning of additional hyperparameters like the decay term."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76ea4c6",
   "metadata": {},
   "source": [
    "### **Summary**:\n",
    "- **BCEWithLogitsLoss** is ideal for binary classification tasks where the outputs are logits, and **Adam** is well-suited for the dynamic and complex landscape of graph data, providing fast convergence and reliable training.\n",
    "- **Alternative loss functions** like **CrossEntropyLoss** or **Hinge Loss** may be useful for different types of classification tasks, but they are generally less efficient or appropriate for binary graph classification tasks.\n",
    "- **Alternative optimizers** like **SGD** or **RMSprop** could work but may require more hyperparameter tuning and could converge more slowly compared to Adam in graph-based models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7c670abd70eec8c8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-10T19:50:08.353147Z",
     "start_time": "2025-03-10T19:50:08.350233Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate(model, loader, evaluator, device):\n",
    "    model.eval()\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            data = data.to(device)\n",
    "            out = model(data.x, data.edge_index, data.batch)\n",
    "            y_true.append(data.y.view(-1, 1))\n",
    "            y_pred.append(out)\n",
    "    y_true = torch.cat(y_true, dim=0)\n",
    "    y_pred = torch.cat(y_pred, dim=0)\n",
    "    return evaluator.eval({'y_true': y_true, 'y_pred': y_pred})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57b8539",
   "metadata": {},
   "source": [
    "## TODO:\n",
    "### What potential pitfalls can arise during the evaluation phase of graph neural networks? (2 points)\n",
    "\n",
    "**ANS:**\n",
    "\n",
    "1. **Data Leakage**:\n",
    "   - One common pitfall in evaluation is **data leakage**, where information from the validation or test set leaks into the model during training, leading to overly optimistic evaluation results. In graph-based models, leakage can occur if the graph structures or node features are inadvertently shared between training and test data (e.g., improper splitting of data, shared node identifiers across splits, etc.).\n",
    "   - **Solution**: Always ensure proper data splits and that the validation/test graphs are completely independent from training graphs. In graph classification tasks, using **graph-level splits** ensures no leakage.\n",
    "\n",
    "2. **Overestimating Model Performance**:\n",
    "   - **Evaluation on the same dataset used for training** or performing **evaluation without proper validation** can result in overly optimistic performance metrics. For example, running evaluations on the training set or testing on graphs that have similar features can lead to inflated metrics.\n",
    "   - **Solution**: Evaluate on a **separate validation/test set** that has not been seen by the model during training. Also, ensure the evaluation follows proper cross-validation procedures where applicable.\n",
    "\n",
    "\n",
    "### How might one detect issues like overfitting or underfitting using evaluation metrics such as ROC-AUC? (2 points)\n",
    "\n",
    "**ANS:**\n",
    "\n",
    "1. **Overfitting**:\n",
    "   - Overfitting occurs when a model performs well on the training data but poorly on the validation/test set. It can be detected by monitoring the **ROC-AUC** (Receiver Operating Characteristic - Area Under Curve) or any other performance metric during training and evaluation.\n",
    "   - **Detection**: If the ROC-AUC on the training set is **significantly higher** than on the validation/test set, this is a clear indication of overfitting. The model has learned the specifics of the training data, but it fails to generalize to unseen data.\n",
    "   - **Solution**: To mitigate overfitting, use techniques like **regularization**, **dropout**, **early stopping**, or **cross-validation** to ensure the model generalizes well.\n",
    "\n",
    "2. **Underfitting**:\n",
    "   - Underfitting occurs when a model fails to learn the underlying patterns in the data, typically due to an overly simplistic model or inadequate training. It can be detected if both the training and validation/test ROC-AUC scores are **significantly low**.\n",
    "   - **Detection**: If the ROC-AUC score is low on both the training and test sets, it indicates that the model has not learned the relevant features, possibly due to insufficient complexity (e.g., too few layers or features) or inadequate training.\n",
    "   - **Solution**: To address underfitting, increase the model complexity (e.g., adding more layers, increasing hidden units), provide better features, or train for more epochs to allow the model to learn more effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab72915f",
   "metadata": {},
   "source": [
    "### **Summary**:\n",
    "- **Pitfalls**: Issues like data leakage or overestimating performance can arise if proper data splits and independent evaluation sets are not used.\n",
    "- **Detection of Overfitting/Underfitting**: Using **ROC-AUC**, overfitting is detected when performance on the training set is much better than on the validation/test set, and underfitting is detected when performance is low on both."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b53dde",
   "metadata": {},
   "source": [
    "### 4.2. Training Loop (Hyper-parameter Tuning to reach `Validation AUC` $\\ge$ 0.7 ) \n",
    "\n",
    "1 pt: `Validation AUC` $\\ge$ 0.69\n",
    "\n",
    "\n",
    "1.5 pts: `Validation AUC` $\\ge$ 0.71\n",
    "\n",
    "\n",
    "2 pts: `Validation AUC` $\\ge$ 0.73"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "80a5d354221345e0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-10T19:50:08.360922Z",
     "start_time": "2025-03-10T19:50:08.356358Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO 11: Tune your hyper-parameter to achieve high performance (2 points)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = GCN(\n",
    "    num_node_features=dataset.num_node_features, \n",
    "    hidden_channels=128,  # Increased hidden units\n",
    "    num_classes=dataset.num_tasks, \n",
    "    dropout_rate=0.3\n",
    ").to(device)\n",
    "evaluator = Evaluator(name='ogbg-molhiv')\n",
    "\n",
    "# Initialize the Adam optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0005, betas=(0.85, 0.999), weight_decay=1e-5)\n",
    "\n",
    "# Learning Rate Scheduler (Cosine Annealing for smooth decay)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=50)\n",
    "\n",
    "# TODO 11: Tune your hyper-parameter to achieve high performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491ea473",
   "metadata": {},
   "source": [
    "### 4.2. Training Loop (Hyper-parameter Tuning to reach `Validation AUC` $\\ge$ 0.7 ) \n",
    "\n",
    "1 pt: `Validation AUC` $\\ge$ 0.69\n",
    "\n",
    "\n",
    "1.5 pts: `Validation AUC` $\\ge$ 0.71\n",
    "\n",
    "\n",
    "2 pts: `Validation AUC` $\\ge$ 0.73"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7f4ade1304643e31",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-03-10T19:50:08.383926Z"
    },
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/150 [00:13<33:54, 13.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Train Loss: 0.1781, Train AUC: 0.4759, Validation AUC: 0.5613\n",
      "Saved best model with Validation AUC: 0.5613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 2/150 [00:21<25:40, 10.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 002, Train Loss: 0.1600, Train AUC: 0.5736, Validation AUC: 0.6429\n",
      "Saved best model with Validation AUC: 0.6429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 3/150 [00:29<22:40,  9.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 003, Train Loss: 0.1597, Train AUC: 0.5815, Validation AUC: 0.6460\n",
      "Saved best model with Validation AUC: 0.6460\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 4/150 [00:36<20:01,  8.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 004, Train Loss: 0.1589, Train AUC: 0.5884, Validation AUC: 0.6470\n",
      "Saved best model with Validation AUC: 0.6470\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 5/150 [00:42<18:20,  7.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 005, Train Loss: 0.1588, Train AUC: 0.5996, Validation AUC: 0.6413\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 6/150 [00:50<18:01,  7.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 006, Train Loss: 0.1588, Train AUC: 0.6090, Validation AUC: 0.6439\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 7/150 [00:56<17:03,  7.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 007, Train Loss: 0.1585, Train AUC: 0.6184, Validation AUC: 0.6440\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 8/150 [01:03<16:46,  7.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 008, Train Loss: 0.1586, Train AUC: 0.6205, Validation AUC: 0.6313\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 9/150 [01:09<16:10,  6.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 009, Train Loss: 0.1583, Train AUC: 0.6212, Validation AUC: 0.6313\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 10/150 [01:16<15:56,  6.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 010, Train Loss: 0.1586, Train AUC: 0.6294, Validation AUC: 0.6340\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 11/150 [01:23<15:40,  6.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 011, Train Loss: 0.1578, Train AUC: 0.6362, Validation AUC: 0.6362\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 12/150 [01:29<15:28,  6.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 012, Train Loss: 0.1580, Train AUC: 0.6468, Validation AUC: 0.6500\n",
      "Saved best model with Validation AUC: 0.6500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▊         | 13/150 [01:36<15:04,  6.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 013, Train Loss: 0.1577, Train AUC: 0.6380, Validation AUC: 0.6336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 14/150 [01:43<15:25,  6.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 014, Train Loss: 0.1579, Train AUC: 0.6345, Validation AUC: 0.6271\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 15/150 [01:51<16:07,  7.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 015, Train Loss: 0.1569, Train AUC: 0.6496, Validation AUC: 0.6424\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 16/150 [01:58<16:01,  7.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 016, Train Loss: 0.1566, Train AUC: 0.6422, Validation AUC: 0.6319\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█▏        | 17/150 [02:05<15:30,  7.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 017, Train Loss: 0.1570, Train AUC: 0.6477, Validation AUC: 0.6344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 18/150 [02:11<14:55,  6.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 018, Train Loss: 0.1574, Train AUC: 0.6405, Validation AUC: 0.6273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 19/150 [02:17<14:26,  6.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 019, Train Loss: 0.1569, Train AUC: 0.6496, Validation AUC: 0.6351\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 20/150 [02:24<14:06,  6.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 020, Train Loss: 0.1566, Train AUC: 0.6467, Validation AUC: 0.6277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 21/150 [02:30<13:56,  6.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 021, Train Loss: 0.1564, Train AUC: 0.6611, Validation AUC: 0.6486\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▍        | 22/150 [02:37<14:00,  6.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 022, Train Loss: 0.1570, Train AUC: 0.6638, Validation AUC: 0.6481\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 23/150 [02:43<13:56,  6.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 023, Train Loss: 0.1562, Train AUC: 0.6609, Validation AUC: 0.6438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 24/150 [02:50<13:33,  6.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 024, Train Loss: 0.1563, Train AUC: 0.6537, Validation AUC: 0.6320\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 25/150 [02:56<13:22,  6.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 025, Train Loss: 0.1556, Train AUC: 0.6530, Validation AUC: 0.6320\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 26/150 [03:02<13:07,  6.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 026, Train Loss: 0.1557, Train AUC: 0.6634, Validation AUC: 0.6428\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 27/150 [03:09<13:27,  6.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 027, Train Loss: 0.1558, Train AUC: 0.6704, Validation AUC: 0.6564\n",
      "Saved best model with Validation AUC: 0.6564\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▊        | 28/150 [03:16<13:45,  6.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 028, Train Loss: 0.1549, Train AUC: 0.6666, Validation AUC: 0.6421\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 29/150 [03:23<13:27,  6.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 029, Train Loss: 0.1555, Train AUC: 0.6703, Validation AUC: 0.6448\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 30/150 [03:29<13:05,  6.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 030, Train Loss: 0.1547, Train AUC: 0.6771, Validation AUC: 0.6507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 31/150 [03:36<13:08,  6.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 031, Train Loss: 0.1547, Train AUC: 0.6795, Validation AUC: 0.6561\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██▏       | 32/150 [03:42<12:51,  6.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 032, Train Loss: 0.1541, Train AUC: 0.6763, Validation AUC: 0.6514\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 33/150 [03:48<12:36,  6.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 033, Train Loss: 0.1543, Train AUC: 0.6705, Validation AUC: 0.6418\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 34/150 [03:55<12:20,  6.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 034, Train Loss: 0.1536, Train AUC: 0.6842, Validation AUC: 0.6510\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 35/150 [04:02<12:46,  6.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 035, Train Loss: 0.1540, Train AUC: 0.6839, Validation AUC: 0.6614\n",
      "Saved best model with Validation AUC: 0.6614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 36/150 [04:08<12:25,  6.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 036, Train Loss: 0.1532, Train AUC: 0.6871, Validation AUC: 0.6642\n",
      "Saved best model with Validation AUC: 0.6642\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▍       | 37/150 [04:14<12:07,  6.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 037, Train Loss: 0.1539, Train AUC: 0.6921, Validation AUC: 0.6699\n",
      "Saved best model with Validation AUC: 0.6699\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 38/150 [04:21<11:48,  6.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 038, Train Loss: 0.1530, Train AUC: 0.6932, Validation AUC: 0.6604\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 39/150 [04:27<11:51,  6.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 039, Train Loss: 0.1526, Train AUC: 0.6886, Validation AUC: 0.6677\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 40/150 [04:33<11:41,  6.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 040, Train Loss: 0.1532, Train AUC: 0.6907, Validation AUC: 0.6559\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 41/150 [04:41<12:03,  6.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 041, Train Loss: 0.1525, Train AUC: 0.6959, Validation AUC: 0.6612\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 42/150 [04:47<12:01,  6.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 042, Train Loss: 0.1524, Train AUC: 0.6956, Validation AUC: 0.6648\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▊       | 43/150 [04:54<11:42,  6.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 043, Train Loss: 0.1519, Train AUC: 0.7001, Validation AUC: 0.6737\n",
      "Saved best model with Validation AUC: 0.6737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 44/150 [05:00<11:23,  6.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 044, Train Loss: 0.1519, Train AUC: 0.6987, Validation AUC: 0.6827\n",
      "Saved best model with Validation AUC: 0.6827\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 45/150 [05:06<11:13,  6.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 045, Train Loss: 0.1519, Train AUC: 0.7014, Validation AUC: 0.6809\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 46/150 [05:14<11:42,  6.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 046, Train Loss: 0.1510, Train AUC: 0.6951, Validation AUC: 0.6635\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███▏      | 47/150 [05:20<11:22,  6.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 047, Train Loss: 0.1512, Train AUC: 0.6979, Validation AUC: 0.6694\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 48/150 [05:27<11:18,  6.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 048, Train Loss: 0.1513, Train AUC: 0.7065, Validation AUC: 0.6838\n",
      "Saved best model with Validation AUC: 0.6838\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 49/150 [05:33<10:58,  6.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 049, Train Loss: 0.1506, Train AUC: 0.7057, Validation AUC: 0.6707\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 50/150 [05:39<10:42,  6.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 050, Train Loss: 0.1502, Train AUC: 0.7054, Validation AUC: 0.6821\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 51/150 [05:45<10:27,  6.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 051, Train Loss: 0.1502, Train AUC: 0.7039, Validation AUC: 0.6696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▍      | 52/150 [05:52<10:16,  6.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 052, Train Loss: 0.1499, Train AUC: 0.7072, Validation AUC: 0.6897\n",
      "Saved best model with Validation AUC: 0.6897\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 53/150 [05:58<10:09,  6.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 053, Train Loss: 0.1503, Train AUC: 0.7096, Validation AUC: 0.6855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 54/150 [06:04<10:12,  6.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 054, Train Loss: 0.1502, Train AUC: 0.7110, Validation AUC: 0.6786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 55/150 [06:11<10:22,  6.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 055, Train Loss: 0.1493, Train AUC: 0.7077, Validation AUC: 0.7004\n",
      "Saved best model with Validation AUC: 0.7004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 56/150 [06:18<10:11,  6.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 056, Train Loss: 0.1492, Train AUC: 0.7127, Validation AUC: 0.6960\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 57/150 [06:25<10:10,  6.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 057, Train Loss: 0.1491, Train AUC: 0.7080, Validation AUC: 0.6953\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▊      | 58/150 [06:31<09:55,  6.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 058, Train Loss: 0.1492, Train AUC: 0.7129, Validation AUC: 0.7012\n",
      "Saved best model with Validation AUC: 0.7012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 59/150 [06:37<09:42,  6.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 059, Train Loss: 0.1489, Train AUC: 0.7145, Validation AUC: 0.7015\n",
      "Saved best model with Validation AUC: 0.7015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 60/150 [06:43<09:32,  6.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 060, Train Loss: 0.1486, Train AUC: 0.7147, Validation AUC: 0.6943\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████      | 61/150 [06:49<09:22,  6.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 061, Train Loss: 0.1493, Train AUC: 0.7122, Validation AUC: 0.6957\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████▏     | 62/150 [06:56<09:32,  6.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 062, Train Loss: 0.1485, Train AUC: 0.7172, Validation AUC: 0.7013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 63/150 [07:03<09:37,  6.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 063, Train Loss: 0.1486, Train AUC: 0.7142, Validation AUC: 0.6843\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 64/150 [07:10<09:21,  6.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 064, Train Loss: 0.1482, Train AUC: 0.7126, Validation AUC: 0.7041\n",
      "Saved best model with Validation AUC: 0.7041\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 65/150 [07:16<09:16,  6.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 065, Train Loss: 0.1485, Train AUC: 0.7165, Validation AUC: 0.7138\n",
      "Saved best model with Validation AUC: 0.7138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 66/150 [07:23<09:03,  6.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 066, Train Loss: 0.1485, Train AUC: 0.7199, Validation AUC: 0.7053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▍     | 67/150 [07:29<08:49,  6.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 067, Train Loss: 0.1483, Train AUC: 0.7194, Validation AUC: 0.7027\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 68/150 [07:35<08:37,  6.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 068, Train Loss: 0.1474, Train AUC: 0.7212, Validation AUC: 0.7091\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 69/150 [07:41<08:29,  6.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 069, Train Loss: 0.1482, Train AUC: 0.7127, Validation AUC: 0.6868\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 70/150 [07:48<08:31,  6.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 070, Train Loss: 0.1485, Train AUC: 0.7220, Validation AUC: 0.6929\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 71/150 [07:54<08:20,  6.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 071, Train Loss: 0.1475, Train AUC: 0.7230, Validation AUC: 0.6951\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 72/150 [08:00<08:13,  6.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 072, Train Loss: 0.1477, Train AUC: 0.7229, Validation AUC: 0.7011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▊     | 73/150 [08:07<08:11,  6.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 073, Train Loss: 0.1473, Train AUC: 0.7243, Validation AUC: 0.6997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▉     | 74/150 [08:14<08:19,  6.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 074, Train Loss: 0.1472, Train AUC: 0.7255, Validation AUC: 0.7063\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 75/150 [08:20<08:16,  6.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 075, Train Loss: 0.1470, Train AUC: 0.7225, Validation AUC: 0.7031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████     | 76/150 [08:27<08:13,  6.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 076, Train Loss: 0.1475, Train AUC: 0.7250, Validation AUC: 0.6973\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████▏    | 77/150 [08:34<08:01,  6.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 077, Train Loss: 0.1466, Train AUC: 0.7264, Validation AUC: 0.7011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 78/150 [08:40<07:59,  6.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 078, Train Loss: 0.1470, Train AUC: 0.7248, Validation AUC: 0.7042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 79/150 [08:47<07:43,  6.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 079, Train Loss: 0.1463, Train AUC: 0.7244, Validation AUC: 0.7117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 80/150 [08:53<07:32,  6.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 080, Train Loss: 0.1461, Train AUC: 0.7250, Validation AUC: 0.7155\n",
      "Saved best model with Validation AUC: 0.7155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 81/150 [08:59<07:20,  6.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 081, Train Loss: 0.1472, Train AUC: 0.7282, Validation AUC: 0.6988\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▍    | 82/150 [09:09<08:24,  7.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 082, Train Loss: 0.1473, Train AUC: 0.7262, Validation AUC: 0.7094\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 83/150 [09:18<08:41,  7.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 083, Train Loss: 0.1470, Train AUC: 0.7280, Validation AUC: 0.6991\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 84/150 [09:24<08:14,  7.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 084, Train Loss: 0.1460, Train AUC: 0.7277, Validation AUC: 0.7151\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 85/150 [09:32<07:58,  7.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 085, Train Loss: 0.1466, Train AUC: 0.7296, Validation AUC: 0.7091\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 86/150 [09:41<08:23,  7.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 086, Train Loss: 0.1460, Train AUC: 0.7283, Validation AUC: 0.7068\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 87/150 [09:49<08:31,  8.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 087, Train Loss: 0.1458, Train AUC: 0.7305, Validation AUC: 0.7147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▊    | 88/150 [09:59<08:46,  8.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 088, Train Loss: 0.1457, Train AUC: 0.7308, Validation AUC: 0.7163\n",
      "Saved best model with Validation AUC: 0.7163\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▉    | 89/150 [10:07<08:40,  8.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 089, Train Loss: 0.1461, Train AUC: 0.7288, Validation AUC: 0.7143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 90/150 [10:17<09:01,  9.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 090, Train Loss: 0.1458, Train AUC: 0.7310, Validation AUC: 0.7047\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████    | 91/150 [10:27<09:01,  9.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 091, Train Loss: 0.1460, Train AUC: 0.7278, Validation AUC: 0.7188\n",
      "Saved best model with Validation AUC: 0.7188\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████▏   | 92/150 [10:39<09:38,  9.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 092, Train Loss: 0.1463, Train AUC: 0.7298, Validation AUC: 0.7187\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 93/150 [10:51<10:06, 10.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 093, Train Loss: 0.1454, Train AUC: 0.7326, Validation AUC: 0.7143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 94/150 [11:02<09:55, 10.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 094, Train Loss: 0.1456, Train AUC: 0.7326, Validation AUC: 0.7180\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 95/150 [11:11<09:29, 10.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 095, Train Loss: 0.1457, Train AUC: 0.7340, Validation AUC: 0.7214\n",
      "Saved best model with Validation AUC: 0.7214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 96/150 [11:23<09:37, 10.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 096, Train Loss: 0.1460, Train AUC: 0.7330, Validation AUC: 0.7182\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▍   | 97/150 [11:34<09:33, 10.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 097, Train Loss: 0.1457, Train AUC: 0.7357, Validation AUC: 0.7133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 98/150 [11:44<09:17, 10.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 098, Train Loss: 0.1455, Train AUC: 0.7348, Validation AUC: 0.7193\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 99/150 [11:53<08:37, 10.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 099, Train Loss: 0.1459, Train AUC: 0.7312, Validation AUC: 0.7103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 100/150 [12:04<08:35, 10.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100, Train Loss: 0.1460, Train AUC: 0.7353, Validation AUC: 0.7151\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 101/150 [12:15<08:37, 10.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 101, Train Loss: 0.1447, Train AUC: 0.7348, Validation AUC: 0.7198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 102/150 [12:24<08:07, 10.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 102, Train Loss: 0.1449, Train AUC: 0.7333, Validation AUC: 0.7126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▊   | 103/150 [12:32<07:20,  9.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 103, Train Loss: 0.1454, Train AUC: 0.7335, Validation AUC: 0.7056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 104/150 [12:39<06:41,  8.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 104, Train Loss: 0.1458, Train AUC: 0.7352, Validation AUC: 0.7219\n",
      "Saved best model with Validation AUC: 0.7219\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 105/150 [12:46<06:03,  8.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 105, Train Loss: 0.1450, Train AUC: 0.7374, Validation AUC: 0.7236\n",
      "Saved best model with Validation AUC: 0.7236\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 106/150 [12:53<05:49,  7.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 106, Train Loss: 0.1454, Train AUC: 0.7387, Validation AUC: 0.7236\n",
      "Saved best model with Validation AUC: 0.7236\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████▏  | 107/150 [13:01<05:39,  7.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 107, Train Loss: 0.1448, Train AUC: 0.7388, Validation AUC: 0.7207\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 108/150 [13:09<05:26,  7.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 108, Train Loss: 0.1447, Train AUC: 0.7379, Validation AUC: 0.7192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 109/150 [13:18<05:36,  8.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 109, Train Loss: 0.1447, Train AUC: 0.7369, Validation AUC: 0.7184\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 110/150 [13:26<05:23,  8.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 110, Train Loss: 0.1448, Train AUC: 0.7384, Validation AUC: 0.7226\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 111/150 [13:33<05:05,  7.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 111, Train Loss: 0.1447, Train AUC: 0.7381, Validation AUC: 0.7249\n",
      "Saved best model with Validation AUC: 0.7249\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▍  | 112/150 [13:40<04:52,  7.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 112, Train Loss: 0.1444, Train AUC: 0.7410, Validation AUC: 0.7199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 113/150 [13:48<04:41,  7.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 113, Train Loss: 0.1448, Train AUC: 0.7388, Validation AUC: 0.7145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 114/150 [13:55<04:34,  7.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 114, Train Loss: 0.1445, Train AUC: 0.7402, Validation AUC: 0.7250\n",
      "Saved best model with Validation AUC: 0.7250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 115/150 [14:03<04:27,  7.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 115, Train Loss: 0.1444, Train AUC: 0.7401, Validation AUC: 0.7229\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 116/150 [14:14<04:52,  8.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 116, Train Loss: 0.1442, Train AUC: 0.7391, Validation AUC: 0.7216\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 117/150 [14:22<04:37,  8.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 117, Train Loss: 0.1449, Train AUC: 0.7402, Validation AUC: 0.7202\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▊  | 118/150 [14:29<04:18,  8.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 118, Train Loss: 0.1444, Train AUC: 0.7415, Validation AUC: 0.7194\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▉  | 119/150 [14:36<03:58,  7.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 119, Train Loss: 0.1451, Train AUC: 0.7437, Validation AUC: 0.7255\n",
      "Saved best model with Validation AUC: 0.7255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 120/150 [14:42<03:39,  7.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 120, Train Loss: 0.1444, Train AUC: 0.7412, Validation AUC: 0.7209\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 121/150 [14:49<03:23,  7.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 121, Train Loss: 0.1444, Train AUC: 0.7414, Validation AUC: 0.7347\n",
      "Saved best model with Validation AUC: 0.7347\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████▏ | 122/150 [14:55<03:12,  6.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 122, Train Loss: 0.1433, Train AUC: 0.7416, Validation AUC: 0.7219\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 123/150 [15:02<03:01,  6.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 123, Train Loss: 0.1435, Train AUC: 0.7445, Validation AUC: 0.7311\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 124/150 [15:10<03:04,  7.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 124, Train Loss: 0.1441, Train AUC: 0.7423, Validation AUC: 0.7451\n",
      "Saved best model with Validation AUC: 0.7451\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 125/150 [15:18<03:07,  7.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 125, Train Loss: 0.1437, Train AUC: 0.7452, Validation AUC: 0.7318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 126/150 [15:24<02:51,  7.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 126, Train Loss: 0.1442, Train AUC: 0.7420, Validation AUC: 0.7413\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▍ | 127/150 [15:31<02:41,  7.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 127, Train Loss: 0.1439, Train AUC: 0.7422, Validation AUC: 0.7268\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 128/150 [15:37<02:31,  6.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 128, Train Loss: 0.1437, Train AUC: 0.7441, Validation AUC: 0.7287\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 129/150 [15:44<02:24,  6.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 129, Train Loss: 0.1444, Train AUC: 0.7467, Validation AUC: 0.7386\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 130/150 [15:51<02:17,  6.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 130, Train Loss: 0.1435, Train AUC: 0.7454, Validation AUC: 0.7282\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 131/150 [15:59<02:15,  7.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 131, Train Loss: 0.1433, Train AUC: 0.7407, Validation AUC: 0.7263\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 132/150 [16:08<02:17,  7.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 132, Train Loss: 0.1441, Train AUC: 0.7438, Validation AUC: 0.7284\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▊ | 133/150 [16:16<02:13,  7.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 133, Train Loss: 0.1434, Train AUC: 0.7433, Validation AUC: 0.7315\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 134/150 [16:24<02:06,  7.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 134, Train Loss: 0.1440, Train AUC: 0.7419, Validation AUC: 0.7359\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 135/150 [16:32<01:58,  7.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 135, Train Loss: 0.1448, Train AUC: 0.7469, Validation AUC: 0.7264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 136/150 [16:41<01:56,  8.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 136, Train Loss: 0.1429, Train AUC: 0.7445, Validation AUC: 0.7343\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████▏| 137/150 [16:49<01:46,  8.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 137, Train Loss: 0.1431, Train AUC: 0.7481, Validation AUC: 0.7387\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 138/150 [16:58<01:38,  8.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 138, Train Loss: 0.1432, Train AUC: 0.7485, Validation AUC: 0.7403\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 139/150 [17:07<01:34,  8.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 139, Train Loss: 0.1430, Train AUC: 0.7493, Validation AUC: 0.7322\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 140/150 [17:17<01:29,  8.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 140, Train Loss: 0.1429, Train AUC: 0.7407, Validation AUC: 0.7255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 141/150 [17:24<01:16,  8.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 141, Train Loss: 0.1431, Train AUC: 0.7472, Validation AUC: 0.7427\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▍| 142/150 [17:31<01:03,  7.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 142, Train Loss: 0.1428, Train AUC: 0.7495, Validation AUC: 0.7251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 143/150 [17:37<00:52,  7.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 143, Train Loss: 0.1426, Train AUC: 0.7476, Validation AUC: 0.7363\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 143/150 [17:44<00:52,  7.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 144, Train Loss: 0.1439, Train AUC: 0.7516, Validation AUC: 0.7373\n",
      "Early stopping at epoch 144 due to no improvement in Validation AUC.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "num_epochs = 150\n",
    "best_valid_auc = 0\n",
    "patience = 20      # Early stopping patience to avoid overfitting\n",
    "trigger_times = 0  # Counter for early stopping\n",
    "\n",
    "for epoch in tqdm(range(1, num_epochs + 1)):\n",
    "    # Training\n",
    "    train_loss = train(model, train_loader, device)\n",
    "\n",
    "    # Evaluation\n",
    "    train_result = evaluate(model, train_loader, evaluator, device)\n",
    "    valid_result = evaluate(model, valid_loader, evaluator, device)\n",
    "\n",
    "    train_auc = train_result['rocauc']\n",
    "    valid_auc = valid_result['rocauc']\n",
    "\n",
    "    # Print metrics\n",
    "    print(f'Epoch: {epoch:03d}, '\n",
    "          f'Train Loss: {train_loss:.4f}, '\n",
    "          f'Train AUC: {train_auc:.4f}, '\n",
    "          f'Validation AUC: {valid_auc:.4f}')\n",
    "\n",
    "    # Save the best model\n",
    "    if valid_auc > best_valid_auc:\n",
    "        best_valid_auc = valid_auc\n",
    "        # Save the model with the best validation AUC\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n",
    "        print(f'Saved best model with Validation AUC: {best_valid_auc:.4f}')\n",
    "        trigger_times = 0  # Reset early stopping trigger\n",
    "    else:\n",
    "        trigger_times += 1  # Increment early stopping trigger\n",
    "    \n",
    "    # Early stopping if validation AUC does not improve for `patience` epochs\n",
    "    if trigger_times >= patience:\n",
    "        print(f\"Early stopping at epoch {epoch} due to no improvement in Validation AUC.\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0fb4dea6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\monat\\AppData\\Local\\Temp\\ipykernel_7432\\675879185.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('best_model.pth'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Test AUC: 0.6865\n"
     ]
    }
   ],
   "source": [
    "# Load the best model for final evaluation\n",
    "model.load_state_dict(torch.load('best_model.pth'))\n",
    "\n",
    "# Final evaluation on the test set\n",
    "test_result = evaluate(model, test_loader, evaluator, device)\n",
    "test_auc = test_result['rocauc']\n",
    "print(f'Final Test AUC: {test_auc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f28247b",
   "metadata": {},
   "source": [
    "### OPEN ENDED QUESTION:\n",
    "#### Reflect on the entire implementation of the GCN model for molecular property prediction.\n",
    "#### - What potential improvements or modifications could you suggest to enhance the model's performance? Consider aspects such as network architecture, hyperparameter tuning, data preprocessing, and evaluation strategies. (2 points)\n",
    "\n",
    "#### **ANS: Potential Improvements & Modifications for Enhancing Performance**\n",
    "Several aspects of the current GCN model can be improved to achieve better performance:\n",
    "\n",
    "1. **Network Architecture Modifications:**\n",
    "   - Introduce **Graph Attention Networks (GAT)** instead of GCN to allow adaptive weighting of neighboring nodes.\n",
    "   - Use **Graph Isomorphism Network (GIN)**, which has been shown to be more powerful in capturing molecular graph structures.\n",
    "   - Add **residual connections** between layers to prevent vanishing gradients and allow deeper architectures.\n",
    "\n",
    "2. **Hyperparameter Tuning:**\n",
    "   - Perform a **grid search** or **Bayesian optimization** over hidden dimensions, learning rate, dropout, and weight decay.\n",
    "   - Try **learning rate warm-up** before applying decay to stabilize training.\n",
    "   - Experiment with **larger batch sizes** (e.g., 512) to achieve smoother gradient updates.\n",
    "\n",
    "3. **Data Preprocessing & Augmentation:**\n",
    "   - Apply **feature engineering** on molecular graphs (e.g., adding edge attributes for bond types).\n",
    "   - Consider **self-supervised pretraining** using contrastive learning techniques like **GraphCL** to improve node representations before fine-tuning.\n",
    "\n",
    "4. **Evaluation Strategies:**\n",
    "   - Use **stratified k-fold cross-validation** to ensure robustness in model evaluation.\n",
    "   - Implement **class balancing techniques** to handle label imbalance in molecular property datasets.\n",
    "   - Monitor **precision-recall curves** in addition to ROC-AUC for better insight into class performance.\n",
    "\n",
    "---\n",
    "#### - How might you leverage additional graph-specific techniques or modern architectures to push the boundaries of performance on this task? (2 points)\n",
    "\n",
    "#### **ANS: Leveraging Additional Graph-Specific Techniques & Modern Architectures**\n",
    "Beyond traditional GCNs, we can incorporate **more advanced graph neural network techniques**:\n",
    "\n",
    "1. **Graph Transformers:**\n",
    "   - Use **Graphormer** or **SAN (Structure-Aware Transformer)** to capture long-range dependencies in molecular graphs.\n",
    "   - These models **outperform standard GNNs** on certain molecular property prediction tasks.\n",
    "\n",
    "2. **Contrastive Learning for Graph Representations:**\n",
    "   - Implement **Graph Contrastive Learning (GraphCL, InfoGraph)** to pretrain the model on unlabeled molecular graphs.\n",
    "   - This can significantly improve feature representations and generalization.\n",
    "\n",
    "3. **Multi-View Graph Learning:**\n",
    "   - Use **multi-scale GCNs** that extract information at **different neighborhood levels**.\n",
    "   - Combine **node-level and subgraph-level embeddings** for richer representations.\n",
    "\n",
    "4. **Combining GNNs with Domain Knowledge:**\n",
    "   - Incorporate **physically meaningful descriptors** (e.g., quantum chemical properties) as input features.\n",
    "   - Hybrid models that integrate **GNNs with traditional cheminformatics features** (e.g., molecular fingerprints) can enhance prediction accuracy.\n",
    "\n",
    "5. **Meta-Learning for Adaptive GNN Training:**\n",
    "   - Use **meta-learning (MAML, Reptile)** to adapt the model quickly to different molecular tasks.\n",
    "   - Helps in few-shot learning scenarios where labeled data is limited.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7283e44f",
   "metadata": {},
   "source": [
    "### **Summary**\n",
    "To push the boundaries of performance, we can:\n",
    "Adopt **advanced GNN architectures** (GAT, GIN, Graph Transformers).  \n",
    "Use **contrastive learning & self-supervised pretraining**.  \n",
    "Explore **multi-scale graph learning** & integrate domain knowledge.  \n",
    "Improve **evaluation strategies & hyperparameter tuning**.\n",
    "\n",
    "By combining these approaches, we can significantly enhance the model's effectiveness for molecular property prediction"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
